{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Defining an Example Model\n",
    "\n",
    "In the next section, we define a simple 2-layer sparse DGP model for a regression task. Weâ€™ll be using this model to demonstrate the usage of the library."
   ],
   "id": "1b1e2221e44c7cac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:36:04.050268Z",
     "start_time": "2024-05-22T23:36:02.893950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "import sparse_dgp as gp\n",
    "from sparse_dgp.layers.linear import LinearReparameterization\n",
    "from sparse_dgp.layers.activations import TMGP"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Defining a 2-layer DTMGP Model\n",
    "\n",
    "First, we define a 2-layer DTMGP model with a single output dimension. The model consists of two layers, each with level-3 sparse grid design."
   ],
   "id": "7767bd7948758eb5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:36:04.056501Z",
     "start_time": "2024-05-22T23:36:04.051242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a 2-layer DTMGP model for regression\n",
    "class SparseDGP_grid(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, design_class, kernel):\n",
    "        super(SparseDGP_grid, self).__init__()\n",
    "        \n",
    "        # 1st layer of DGP: input:[n, input_dim] size tensor, output:[n, w1] size tensor\n",
    "        self.tmk1 = TMGP(in_features=input_dim, n_level=3, design_class=design_class, kernel=kernel)\n",
    "        self.fc1 = LinearReparameterization(\n",
    "            in_features=self.tmk1.out_features, \n",
    "            out_features=8, \n",
    "            prior_mean=0.0, \n",
    "            prior_variance=1.0, \n",
    "            posterior_mu_init=0.0, \n",
    "            posterior_rho_init=-3.0, \n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        # 2nd layer of DGP: input:[n, w1] size tensor, output:[n, output_dim] size tensor\n",
    "        self.tmk2 = TMGP(in_features=8, n_level=3, design_class=design_class, kernel=kernel)\n",
    "        self.fc2 = LinearReparameterization(\n",
    "            in_features=self.tmk2.out_features, \n",
    "            out_features=output_dim, \n",
    "            prior_mean=0.0, \n",
    "            prior_variance=1.0, \n",
    "            posterior_mu_init=0.0, \n",
    "            posterior_rho_init=-3.0, \n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        kl_sum = 0\n",
    "\n",
    "        x = self.tmk1(x)\n",
    "        x, kl = self.fc1(x)\n",
    "        kl_sum += kl\n",
    "\n",
    "        x = self.tmk2(x)\n",
    "        x, kl = self.fc2(x)\n",
    "        kl_sum += kl\n",
    "\n",
    "        return torch.squeeze(x), kl_sum"
   ],
   "id": "5f19e34aab5634b8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We set up the training data for this example. We'll be using 1000 regularly spaced points in the range [0, 10] as input data. The output data is generated by a function that takes the input data and adds Gaussian noise to get the training labels."
   ],
   "id": "4681af98afe7c813"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:36:04.065586Z",
     "start_time": "2024-05-22T23:36:04.056501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_X = torch.linspace(0, 1, 1000)\n",
    "train_y = 3 * train_X + 2 + torch.randn(train_X.size()) * 0.1\n",
    "\n",
    "class RegressionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "dataset = RegressionDataset(train_X, train_y)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)"
   ],
   "id": "ae83117d86eb8504",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Initializing the Model and the Optimizer",
   "id": "73f0a7e87bd85a2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:36:05.548184Z",
     "start_time": "2024-05-22T23:36:04.066287Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sparse_dgp.utils.sparse_activation.design_class import HyperbolicCrossDesign\n",
    "from sparse_dgp.kernels.laplace_kernel import LaplaceProductKernel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using: \", device)\n",
    "\n",
    "model = SparseDGP_grid(input_dim=1, \n",
    "                       output_dim=1, \n",
    "                       design_class=HyperbolicCrossDesign, \n",
    "                       kernel=LaplaceProductKernel(1.),\n",
    "                       ).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ],
   "id": "60d11360be6a6280",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Training the Model\n",
    "\n",
    "In the next cell, we handle using variational inference (VI) to train the 2-layer sparse DGP model."
   ],
   "id": "440609ecdd176828"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T23:36:15.507400Z",
     "start_time": "2024-05-22T23:36:05.549198Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output_ = []\n",
    "        kl_ = []\n",
    "        for mc_run in range(1):\n",
    "            output, kl = model(data)\n",
    "            output_.append(output)\n",
    "            kl_.append(kl)\n",
    "        output = torch.mean(torch.stack(output_), dim=0)\n",
    "        kl = torch.mean(torch.stack(kl_), dim=0)\n",
    "        nll_loss = F.mse_loss(output, target)\n",
    "        # ELBO loss\n",
    "        loss = nll_loss + (kl / 32)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")"
   ],
   "id": "8f8ef20ddad946c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.766799449920654\n",
      "Epoch: 1, Loss: 0.804113507270813\n",
      "Epoch: 2, Loss: 0.4882166087627411\n",
      "Epoch: 3, Loss: 0.4709911346435547\n",
      "Epoch: 4, Loss: 0.484585702419281\n",
      "Epoch: 5, Loss: 0.3999895453453064\n",
      "Epoch: 6, Loss: 0.5632978677749634\n",
      "Epoch: 7, Loss: 0.44556406140327454\n",
      "Epoch: 8, Loss: 0.36538970470428467\n",
      "Epoch: 9, Loss: 0.6838325262069702\n",
      "Epoch: 10, Loss: 0.3134954571723938\n",
      "Epoch: 11, Loss: 0.4529147744178772\n",
      "Epoch: 12, Loss: 0.34818488359451294\n",
      "Epoch: 13, Loss: 0.31457337737083435\n",
      "Epoch: 14, Loss: 0.35485103726387024\n",
      "Epoch: 15, Loss: 0.7038782835006714\n",
      "Epoch: 16, Loss: 0.260682076215744\n",
      "Epoch: 17, Loss: 0.3148376941680908\n",
      "Epoch: 18, Loss: 0.3209889233112335\n",
      "Epoch: 19, Loss: 0.2719704210758209\n",
      "Epoch: 20, Loss: 0.2749040722846985\n",
      "Epoch: 21, Loss: 0.4783743619918823\n",
      "Epoch: 22, Loss: 0.3018037676811218\n",
      "Epoch: 23, Loss: 0.3293628692626953\n",
      "Epoch: 24, Loss: 0.9386325478553772\n",
      "Epoch: 25, Loss: 0.25550228357315063\n",
      "Epoch: 26, Loss: 0.3010644018650055\n",
      "Epoch: 27, Loss: 0.26327937841415405\n",
      "Epoch: 28, Loss: 0.3248582184314728\n",
      "Epoch: 29, Loss: 0.46342402696609497\n",
      "Epoch: 30, Loss: 0.36582863330841064\n",
      "Epoch: 31, Loss: 0.24253560602664948\n",
      "Epoch: 32, Loss: 0.25111255049705505\n",
      "Epoch: 33, Loss: 0.2713894248008728\n",
      "Epoch: 34, Loss: 0.3093208372592926\n",
      "Epoch: 35, Loss: 0.2792777717113495\n",
      "Epoch: 36, Loss: 0.24135246872901917\n",
      "Epoch: 37, Loss: 0.2585584223270416\n",
      "Epoch: 38, Loss: 0.23651090264320374\n",
      "Epoch: 39, Loss: 0.23969897627830505\n",
      "Epoch: 40, Loss: 0.4844249188899994\n",
      "Epoch: 41, Loss: 0.24768288433551788\n",
      "Epoch: 42, Loss: 0.5187532901763916\n",
      "Epoch: 43, Loss: 0.24880248308181763\n",
      "Epoch: 44, Loss: 0.2373906373977661\n",
      "Epoch: 45, Loss: 0.2693292200565338\n",
      "Epoch: 46, Loss: 0.21754342317581177\n",
      "Epoch: 47, Loss: 0.2127612680196762\n",
      "Epoch: 48, Loss: 0.3019932806491852\n",
      "Epoch: 49, Loss: 0.2979494631290436\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "See our [documentation](https://sparse-dgp.readthedocs.io/en/latest/) for more information on how to use the library.",
   "id": "1ca660770bc27323"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
